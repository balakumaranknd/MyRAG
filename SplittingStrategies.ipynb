{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdf1dee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "# Introduction to Artificial Intelligence\n",
    "\n",
    "Artificial Intelligence (AI) is the simulation of human intelligence in machines. These machines are programmed to think and learn like humans. The field was founded in 1956 at Dartmouth College.\n",
    "\n",
    "## Machine Learning\n",
    "\n",
    "Machine learning is a subset of AI. It allows systems to learn from data without being explicitly programmed. There are three main types of machine learning.\n",
    "\n",
    "Supervised learning uses labelled data to train models. The model learns to map inputs to outputs. Examples include classification and regression tasks.\n",
    "\n",
    "Unsupervised learning finds hidden patterns in unlabelled data. Clustering and dimensionality reduction are common techniques. K-means and PCA are popular algorithms.\n",
    "\n",
    "Reinforcement learning trains agents through rewards and penalties. The agent learns by interacting with an environment. AlphaGo used reinforcement learning to beat world champions.\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "Deep learning uses neural networks with many layers. It has revolutionised computer vision and natural language processing. GPU computing made deep learning practical.\n",
    "\n",
    "Convolutional Neural Networks (CNNs) excel at image recognition. They use filters to detect features like edges and shapes. ResNet and VGG are famous CNN architectures.\n",
    "\n",
    "Recurrent Neural Networks (RNNs) handle sequential data. They have memory that persists across time steps. LSTMs solved the vanishing gradient problem in RNNs.\n",
    "\n",
    "## Natural Language Processing\n",
    "\n",
    "NLP enables computers to understand human language. It powers applications like chatbots, translation, and sentiment analysis. BERT and GPT transformed the NLP landscape.\n",
    "\n",
    "Tokenisation breaks text into smaller units called tokens. It is the first step in most NLP pipelines. Words, subwords, or characters can all be tokens.\n",
    "\n",
    "Word embeddings represent words as dense vectors. Similar words have similar vectors. Word2Vec and GloVe are classic embedding models.\n",
    "\n",
    "## Applications\n",
    "\n",
    "AI is used in healthcare to detect diseases from medical images. It helps doctors make faster and more accurate diagnoses. IBM Watson was one of the first AI systems in healthcare.\n",
    "\n",
    "Self-driving cars use AI to perceive and navigate the environment. Sensors, cameras, and radar feed data into AI models. Tesla and Waymo are leaders in autonomous vehicles.\n",
    "\n",
    "Recommendation systems power Netflix, Spotify, and Amazon. They analyse user behaviour to suggest relevant content. Collaborative filtering is a common recommendation technique.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afcc66b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [Document(page_content=sample_text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e2bfd1",
   "metadata": {},
   "source": [
    "#### Exercise 1 : Character Splitting\n",
    "\n",
    "Split the text using \".\" as the seperator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "818ea1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 124, which is longer than the specified 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Chunks\t\t: 39\n",
      "\n",
      "First Chunk\t\t: page_content='# Introduction to Artificial Intelligence\n",
      "\n",
      "Artificial Intelligence (AI) is the simulation of human intelligence in machines'\n",
      "\n",
      " Chunk Sizes\t\t: [123, 60, 50, 55, 72, 46, 97, 52, 62, 61, 38, 66, 51, 59, 69, 69, 42, 63, 57, 43, 55, 48, 51, 82, 73, 42, 57, 92, 84, 47, 80, 56, 56, 65, 52, 50, 57, 55, 61]\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator=\".\",\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "char_chunks = splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Number of Chunks\\t\\t: {len(char_chunks)}\")\n",
    "print(f\"\\nFirst Chunk\\t\\t: {char_chunks[0]}\")\n",
    "print(f\"\\n Chunk Sizes\\t\\t: {[len(c.page_content) for c in char_chunks]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56504d71",
   "metadata": {},
   "source": [
    "#### Exercise 2 : Recursive Character Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "67b9acde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Chunks\t\t: 15\n",
      "\n",
      "First 3 Chunks\t\t:\n",
      "\n",
      " --- Chunk 1 (41 characters): --- \n",
      "\n",
      "# Introduction to Artificial Intelligence\n",
      "\n",
      " --- Chunk 2 (195 characters): --- \n",
      "\n",
      "Artificial Intelligence (AI) is the simulation of human intelligence in machines. These machines are programmed to think and learn like humans. The field was founded in 1956 at Dartmouth College.\n",
      "\n",
      " --- Chunk 3 (178 characters): --- \n",
      "\n",
      "## Machine Learning\n",
      "\n",
      "Machine learning is a subset of AI. It allows systems to learn from data without being explicitly programmed. There are three main types of machine learning.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "rec_chunks = splitter.split_documents(documents)\n",
    "    \n",
    "print(f\"Number of Chunks\\t\\t: {len(rec_chunks)}\")\n",
    "print(f\"\\nFirst 3 Chunks\\t\\t:\")\n",
    "for i, chunk in enumerate(rec_chunks[:3]):\n",
    "    print(f\"\\n --- Chunk {i+1} ({len(chunk.page_content)} characters): --- \\n\")\n",
    "    print(chunk.page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df75588d",
   "metadata": {},
   "source": [
    "#### Exercise 3: Token Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3d920daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 3\n",
      "\n",
      "First chunk:\n",
      "\n",
      "# Introduction to Artificial Intelligence\n",
      "\n",
      "Artificial Intelligence (AI) is the simulation of human intelligence in machines. These machines are programmed to think and learn like humans. The field was founded in 1956 at Dartmouth College.\n",
      "\n",
      "## Machine Learning\n",
      "\n",
      "Machine learning is a subset of AI. It allows systems to learn from data without being explicitly programmed. There are three main types of machine learning.\n",
      "\n",
      "Supervised learning uses labelled data to train models. The model learns to map inputs to outputs. Examples include classification and regression tasks.\n",
      "\n",
      "Unsupervised learning finds hidden patterns in unlabelled data. Clustering and dimensionality reduction are common techniques. K-means and PCA are popular algorithms.\n",
      "\n",
      "Reinforcement learning trains agents through rewards and penalties. The agent learns by interacting with an environment. AlphaGo used reinforcement learning to beat world champions.\n",
      "\n",
      "## Deep Learning\n",
      "\n",
      "Deep learning uses neural networks with many layers. It has revolutionised computer vision and\n",
      "\n",
      "Token chunk sizes (chars): [1038, 959, 697]\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "# your code here\n",
    "splitter = TokenTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "tok_chunks = splitter.split_text(sample_text)\n",
    "\n",
    "print(f\"Number of chunks: {len(tok_chunks)}\")\n",
    "print(f\"\\nFirst chunk:\\n{tok_chunks[0]}\")\n",
    "print(f\"\\nToken chunk sizes (chars): {[len(c) for c in tok_chunks]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edd08f7",
   "metadata": {},
   "source": [
    "#### Exercise 4: Markdown Header Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c606377f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 5\n",
      "\n",
      "--- Metadata: {'Title': 'Introduction to Artificial Intelligence'} ---)\n",
      "\n",
      "Artificial Intelligence (AI) is the simulation of human intelligence in machines. These machines are programmed to think and learn like humans. The field was founded in 1956 at Dartmouth College.\n",
      "\n",
      "--- Metadata: {'Title': 'Introduction to Artificial Intelligence', 'Section': 'Machine Learning'} ---)\n",
      "\n",
      "Machine learning is a subset of AI. It allows systems to learn from data without being explicitly programmed. There are three main types of machine learning.  \n",
      "Supervised learning uses labelled data t\n",
      "\n",
      "--- Metadata: {'Title': 'Introduction to Artificial Intelligence', 'Section': 'Deep Learning'} ---)\n",
      "\n",
      "Deep learning uses neural networks with many layers. It has revolutionised computer vision and natural language processing. GPU computing made deep learning practical.  \n",
      "Convolutional Neural Networks \n",
      "\n",
      "--- Metadata: {'Title': 'Introduction to Artificial Intelligence', 'Section': 'Natural Language Processing'} ---)\n",
      "\n",
      "NLP enables computers to understand human language. It powers applications like chatbots, translation, and sentiment analysis. BERT and GPT transformed the NLP landscape.  \n",
      "Tokenisation breaks text in\n",
      "\n",
      "--- Metadata: {'Title': 'Introduction to Artificial Intelligence', 'Section': 'Applications'} ---)\n",
      "\n",
      "AI is used in healthcare to detect diseases from medical images. It helps doctors make faster and more accurate diagnoses. IBM Watson was one of the first AI systems in healthcare.  \n",
      "Self-driving cars\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "headers = [(\"#\",\"Title\"),\n",
    "           (\"##\",\"Section\"), \n",
    "        ]\n",
    "\n",
    "splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers)\n",
    "mark_chunks = splitter.split_text(sample_text)\n",
    "\n",
    "print(f\"Number of chunks: {len(mark_chunks)}\")\n",
    "for chunk in mark_chunks:\n",
    "    print(f\"\\n--- Metadata: {chunk.metadata} ---)\\n\")\n",
    "    print(chunk.page_content[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde47fbd",
   "metadata": {},
   "source": [
    "#### Exercise 5: Code Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4498c22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Chunks : 3\n",
      "\n",
      "--- Chunk 1 (241 characters) ---\n",
      "\n",
      "def load_documents(path):\n",
      "    loader = PyPDFLoader(path)\n",
      "    return loader.load()\n",
      "\n",
      "def split_documents(documents):\n",
      "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
      "    return splitter.split_documents(documents)\n",
      "\n",
      "--- Chunk 2 (95 characters) ---\n",
      "\n",
      "def create_vectorstore(chunks, embeddings):\n",
      "    return FAISS.from_documents(chunks, embeddings)\n",
      "\n",
      "--- Chunk 3 (252 characters) ---\n",
      "\n",
      "def create_chain(vectorstore, prompt, llm):\n",
      "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
      "    return (\n",
      "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
      "        | prompt\n",
      "        | llm\n",
      "        | StrOutputParser()\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import Language, RecursiveCharacterTextSplitter\n",
    "\n",
    "code = \"\"\"\n",
    "def load_documents(path):\n",
    "    loader = PyPDFLoader(path)\n",
    "    return loader.load()\n",
    "\n",
    "def split_documents(documents):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    return splitter.split_documents(documents)\n",
    "\n",
    "def create_vectorstore(chunks, embeddings):\n",
    "    return FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "def create_chain(vectorstore, prompt, llm):\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "    return (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "lang_chunks = splitter.split_text(code)\n",
    "\n",
    "print(f\"Number of Chunks : {len(lang_chunks)}\")\n",
    "for i, chunk in enumerate(lang_chunks):\n",
    "    print(f\"\\n--- Chunk {i+1} ({len(chunk)} characters) ---\\n\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d50835",
   "metadata": {},
   "source": [
    "#### Exercise 6: Semantic Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "460ac9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ba25539b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 14\n",
      "\n",
      "--- Chunk 1 (187 characters) ---\n",
      "\n",
      "\n",
      "# Introduction to Artificial Intelligence\n",
      "\n",
      "Artificial Intelligence (AI) is the simulation of human intelligence in machines. These machines are programmed to think and learn like humans.\n",
      "\n",
      "--- Chunk 2 (108 characters) ---\n",
      "\n",
      "The field was founded in 1956 at Dartmouth College. ## Machine Learning\n",
      "\n",
      "Machine learning is a subset of AI.\n",
      "\n",
      "--- Chunk 3 (73 characters) ---\n",
      "\n",
      "It allows systems to learn from data without being explicitly programmed.\n",
      "\n",
      "--- Chunk 4 (327 characters) ---\n",
      "\n",
      "There are three main types of machine learning. Supervised learning uses labelled data to train models. The model learns to map inputs to outputs. Examples include classification and regression tasks. Unsupervised learning finds hidden patterns in unlabelled data. Clustering and dimensionality reduc\n",
      "\n",
      "--- Chunk 5 (107 characters) ---\n",
      "\n",
      "K-means and PCA are popular algorithms. Reinforcement learning trains agents through rewards and penalties.\n",
      "\n",
      "--- Chunk 6 (184 characters) ---\n",
      "\n",
      "The agent learns by interacting with an environment. AlphaGo used reinforcement learning to beat world champions. ## Deep Learning\n",
      "\n",
      "Deep learning uses neural networks with many layers.\n",
      "\n",
      "--- Chunk 7 (340 characters) ---\n",
      "\n",
      "It has revolutionised computer vision and natural language processing. GPU computing made deep learning practical. Convolutional Neural Networks (CNNs) excel at image recognition. They use filters to detect features like edges and shapes. ResNet and VGG are famous CNN architectures. Recurrent Neural\n",
      "\n",
      "--- Chunk 8 (186 characters) ---\n",
      "\n",
      "They have memory that persists across time steps. LSTMs solved the vanishing gradient problem in RNNs. ## Natural Language Processing\n",
      "\n",
      "NLP enables computers to understand human language.\n",
      "\n",
      "--- Chunk 9 (221 characters) ---\n",
      "\n",
      "It powers applications like chatbots, translation, and sentiment analysis. BERT and GPT transformed the NLP landscape. Tokenisation breaks text into smaller units called tokens. It is the first step in most NLP pipelines.\n",
      "\n",
      "--- Chunk 10 (135 characters) ---\n",
      "\n",
      "Words, subwords, or characters can all be tokens. Word embeddings represent words as dense vectors. Similar words have similar vectors.\n",
      "\n",
      "--- Chunk 11 (130 characters) ---\n",
      "\n",
      "Word2Vec and GloVe are classic embedding models. ## Applications\n",
      "\n",
      "AI is used in healthcare to detect diseases from medical images.\n",
      "\n",
      "--- Chunk 12 (182 characters) ---\n",
      "\n",
      "It helps doctors make faster and more accurate diagnoses. IBM Watson was one of the first AI systems in healthcare. Self-driving cars use AI to perceive and navigate the environment.\n",
      "\n",
      "--- Chunk 13 (164 characters) ---\n",
      "\n",
      "Sensors, cameras, and radar feed data into AI models. Tesla and Waymo are leaders in autonomous vehicles. Recommendation systems power Netflix, Spotify, and Amazon.\n",
      "\n",
      "--- Chunk 14 (119 characters) ---\n",
      "\n",
      "They analyse user behaviour to suggest relevant content. Collaborative filtering is a common recommendation technique. \n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=openai_api_key)\n",
    "\n",
    "splitter = SemanticChunker(embeddings=embeddings, \n",
    "                           breakpoint_threshold_type=\"percentile\",\n",
    "                           breakpoint_threshold_amount=70)\n",
    "\n",
    "semantic_chunks = splitter.split_text(sample_text)\n",
    "\n",
    "print(f\"Number of chunks: {len(semantic_chunks)}\")\n",
    "for i, chunk in enumerate(semantic_chunks):\n",
    "    print(f\"\\n--- Chunk {i+1} ({len(chunk)} characters) ---\\n\")\n",
    "    print(chunk[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2cd262",
   "metadata": {},
   "source": [
    "#### Exercise 7: Compare All Methods Side by Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d303b6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method            Chunks   Avg Size    Min    Max\n",
      "--------------------------------------------------\n",
      "Character             39         61     38    123\n",
      "Recursive             15        165     41    197\n",
      "Token                  3        898    697   1038\n",
      "Markdown               5        471    195    665\n",
      "Semantic              14        175     73    340\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    \"Character\": None,\n",
    "    \"Recursive\": None,\n",
    "    \"Token\": None,\n",
    "    \"Markdown\": None,\n",
    "    \"Semantic\": None\n",
    "}\n",
    "\n",
    "# fill in each with your chunks from above\n",
    "results[\"Character\"]  = char_chunks\n",
    "results[\"Recursive\"]  = rec_chunks\n",
    "results[\"Token\"]      = tok_chunks\n",
    "results[\"Markdown\"]   = mark_chunks\n",
    "results[\"Semantic\"]   = semantic_chunks\n",
    "\n",
    "print(f\"{'Method':<15} {'Chunks':>8} {'Avg Size':>10} {'Min':>6} {'Max':>6}\")\n",
    "print(\"-\" * 50)\n",
    "for method, chunks in results.items():\n",
    "    sizes = [len(c) if isinstance(c, str) else len(c.page_content) for c in chunks]\n",
    "    print(f\"{method:<15} {len(chunks):>8} {sum(sizes)//len(sizes):>10} {min(sizes):>6} {max(sizes):>6}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26ab5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37de8eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
